{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Filename: threshold_learning_demo.ipynb\n",
    "#\n",
    "# Purpose: Demonstrate the use of the 'threshold_learning' library for multi-label\n",
    "#          classification tasks.\n",
    "#\n",
    "# Author(s): Bobby (Robert) Lumpkin\n",
    "#\n",
    "# Library Dependencies: numpy, pandas, tensorflow, bpmll, threshold_learning\n",
    "########################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold Learning Demonstration for Multi-Label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary modules\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from bpmll import bp_mll_loss\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prep the Data\n",
    "\n",
    "Let's start by loading in our data. We'll be using the `Yeast` dataset which can be found <a href = \"http://www.uco.es/kdis/mllresources/\">here</a>. As the description from the link states: \"this dataset contains micro-array expressions and phylogenetic profiles for 2417 yeast genes. Each gene is annotated with a subset of 14 functional categories (e.g. Metabolism, energy, etc.) of the top level of the functional catalogue.\". The training set is loaded as a pandas datarame, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Att1</th>\n",
       "      <th>Att2</th>\n",
       "      <th>Att3</th>\n",
       "      <th>Att4</th>\n",
       "      <th>Att5</th>\n",
       "      <th>Att6</th>\n",
       "      <th>Att7</th>\n",
       "      <th>Att8</th>\n",
       "      <th>Att9</th>\n",
       "      <th>Att10</th>\n",
       "      <th>...</th>\n",
       "      <th>Class5</th>\n",
       "      <th>Class6</th>\n",
       "      <th>Class7</th>\n",
       "      <th>Class8</th>\n",
       "      <th>Class9</th>\n",
       "      <th>Class10</th>\n",
       "      <th>Class11</th>\n",
       "      <th>Class12</th>\n",
       "      <th>Class13</th>\n",
       "      <th>Class14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.093700</td>\n",
       "      <td>0.139771</td>\n",
       "      <td>0.062774</td>\n",
       "      <td>0.007698</td>\n",
       "      <td>0.083873</td>\n",
       "      <td>-0.119156</td>\n",
       "      <td>0.073305</td>\n",
       "      <td>0.005510</td>\n",
       "      <td>0.027523</td>\n",
       "      <td>0.043477</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.022711</td>\n",
       "      <td>-0.050504</td>\n",
       "      <td>-0.035691</td>\n",
       "      <td>-0.065434</td>\n",
       "      <td>-0.084316</td>\n",
       "      <td>-0.378560</td>\n",
       "      <td>0.038212</td>\n",
       "      <td>0.085770</td>\n",
       "      <td>0.182613</td>\n",
       "      <td>-0.055544</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.090407</td>\n",
       "      <td>0.021198</td>\n",
       "      <td>0.208712</td>\n",
       "      <td>0.102752</td>\n",
       "      <td>0.119315</td>\n",
       "      <td>0.041729</td>\n",
       "      <td>-0.021728</td>\n",
       "      <td>0.019603</td>\n",
       "      <td>-0.063853</td>\n",
       "      <td>-0.053756</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.085235</td>\n",
       "      <td>0.009540</td>\n",
       "      <td>-0.013228</td>\n",
       "      <td>0.094063</td>\n",
       "      <td>-0.013592</td>\n",
       "      <td>-0.030719</td>\n",
       "      <td>-0.116062</td>\n",
       "      <td>-0.131674</td>\n",
       "      <td>-0.165448</td>\n",
       "      <td>-0.123053</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'1'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.088765</td>\n",
       "      <td>-0.026743</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>-0.043819</td>\n",
       "      <td>-0.005465</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>-0.055865</td>\n",
       "      <td>-0.071484</td>\n",
       "      <td>-0.159025</td>\n",
       "      <td>-0.111348</td>\n",
       "      <td>...</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "      <td>b'0'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Att1      Att2      Att3      Att4      Att5      Att6      Att7  \\\n",
       "0  0.093700  0.139771  0.062774  0.007698  0.083873 -0.119156  0.073305   \n",
       "1 -0.022711 -0.050504 -0.035691 -0.065434 -0.084316 -0.378560  0.038212   \n",
       "2 -0.090407  0.021198  0.208712  0.102752  0.119315  0.041729 -0.021728   \n",
       "3 -0.085235  0.009540 -0.013228  0.094063 -0.013592 -0.030719 -0.116062   \n",
       "4 -0.088765 -0.026743  0.002075 -0.043819 -0.005465  0.004306 -0.055865   \n",
       "\n",
       "       Att8      Att9     Att10  ...  Class5  Class6  Class7  Class8  Class9  \\\n",
       "0  0.005510  0.027523  0.043477  ...    b'0'    b'0'    b'0'    b'0'    b'0'   \n",
       "1  0.085770  0.182613 -0.055544  ...    b'0'    b'0'    b'1'    b'1'    b'0'   \n",
       "2  0.019603 -0.063853 -0.053756  ...    b'0'    b'0'    b'0'    b'0'    b'0'   \n",
       "3 -0.131674 -0.165448 -0.123053  ...    b'0'    b'0'    b'0'    b'0'    b'0'   \n",
       "4 -0.071484 -0.159025 -0.111348  ...    b'0'    b'0'    b'0'    b'0'    b'0'   \n",
       "\n",
       "   Class10  Class11  Class12  Class13  Class14  \n",
       "0     b'0'     b'0'     b'0'     b'0'     b'0'  \n",
       "1     b'0'     b'0'     b'1'     b'1'     b'0'  \n",
       "2     b'0'     b'0'     b'1'     b'1'     b'0'  \n",
       "3     b'0'     b'0'     b'1'     b'1'     b'1'  \n",
       "4     b'0'     b'0'     b'0'     b'0'     b'0'  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the 'Yeast' dataset\n",
    "data_train = arff.loadarff('Yeast/Yeast-train.arff')\n",
    "df_train = pd.DataFrame(data_train[0])\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion to Floats & Numpy Arrays \n",
    "The label indicators are loaded as bytes literals. We'll convert them to floats and generate numpy arrays of covariate and label values, to be used for training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Att1</th>\n",
       "      <th>Att2</th>\n",
       "      <th>Att3</th>\n",
       "      <th>Att4</th>\n",
       "      <th>Att5</th>\n",
       "      <th>Att6</th>\n",
       "      <th>Att7</th>\n",
       "      <th>Att8</th>\n",
       "      <th>Att9</th>\n",
       "      <th>Att10</th>\n",
       "      <th>...</th>\n",
       "      <th>Class5</th>\n",
       "      <th>Class6</th>\n",
       "      <th>Class7</th>\n",
       "      <th>Class8</th>\n",
       "      <th>Class9</th>\n",
       "      <th>Class10</th>\n",
       "      <th>Class11</th>\n",
       "      <th>Class12</th>\n",
       "      <th>Class13</th>\n",
       "      <th>Class14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.093700</td>\n",
       "      <td>0.139771</td>\n",
       "      <td>0.062774</td>\n",
       "      <td>0.007698</td>\n",
       "      <td>0.083873</td>\n",
       "      <td>-0.119156</td>\n",
       "      <td>0.073305</td>\n",
       "      <td>0.005510</td>\n",
       "      <td>0.027523</td>\n",
       "      <td>0.043477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.022711</td>\n",
       "      <td>-0.050504</td>\n",
       "      <td>-0.035691</td>\n",
       "      <td>-0.065434</td>\n",
       "      <td>-0.084316</td>\n",
       "      <td>-0.378560</td>\n",
       "      <td>0.038212</td>\n",
       "      <td>0.085770</td>\n",
       "      <td>0.182613</td>\n",
       "      <td>-0.055544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.090407</td>\n",
       "      <td>0.021198</td>\n",
       "      <td>0.208712</td>\n",
       "      <td>0.102752</td>\n",
       "      <td>0.119315</td>\n",
       "      <td>0.041729</td>\n",
       "      <td>-0.021728</td>\n",
       "      <td>0.019603</td>\n",
       "      <td>-0.063853</td>\n",
       "      <td>-0.053756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.085235</td>\n",
       "      <td>0.009540</td>\n",
       "      <td>-0.013228</td>\n",
       "      <td>0.094063</td>\n",
       "      <td>-0.013592</td>\n",
       "      <td>-0.030719</td>\n",
       "      <td>-0.116062</td>\n",
       "      <td>-0.131674</td>\n",
       "      <td>-0.165448</td>\n",
       "      <td>-0.123053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.088765</td>\n",
       "      <td>-0.026743</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>-0.043819</td>\n",
       "      <td>-0.005465</td>\n",
       "      <td>0.004306</td>\n",
       "      <td>-0.055865</td>\n",
       "      <td>-0.071484</td>\n",
       "      <td>-0.159025</td>\n",
       "      <td>-0.111348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Att1      Att2      Att3      Att4      Att5      Att6      Att7  \\\n",
       "0  0.093700  0.139771  0.062774  0.007698  0.083873 -0.119156  0.073305   \n",
       "1 -0.022711 -0.050504 -0.035691 -0.065434 -0.084316 -0.378560  0.038212   \n",
       "2 -0.090407  0.021198  0.208712  0.102752  0.119315  0.041729 -0.021728   \n",
       "3 -0.085235  0.009540 -0.013228  0.094063 -0.013592 -0.030719 -0.116062   \n",
       "4 -0.088765 -0.026743  0.002075 -0.043819 -0.005465  0.004306 -0.055865   \n",
       "\n",
       "       Att8      Att9     Att10  ...  Class5  Class6  Class7  Class8  Class9  \\\n",
       "0  0.005510  0.027523  0.043477  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "1  0.085770  0.182613 -0.055544  ...     0.0     0.0     1.0     1.0     0.0   \n",
       "2  0.019603 -0.063853 -0.053756  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "3 -0.131674 -0.165448 -0.123053  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "4 -0.071484 -0.159025 -0.111348  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   Class10  Class11  Class12  Class13  Class14  \n",
       "0      0.0      0.0      0.0      0.0      0.0  \n",
       "1      0.0      0.0      1.0      1.0      0.0  \n",
       "2      0.0      0.0      1.0      1.0      0.0  \n",
       "3      0.0      0.0      1.0      1.0      1.0  \n",
       "4      0.0      0.0      0.0      0.0      0.0  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Convert the class labels into floats\n",
    "label_names = []\n",
    "for name in df_train.columns:\n",
    "    if \"Class\" in name:\n",
    "        label_names.append(name)\n",
    "df_train[label_names] = df_train[label_names].astype(\"float\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[np.setdiff1d(df_train.columns, label_names)].to_numpy()\n",
    "Y_train = df_train[label_names].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Train a Network Using Cross Entropy Loss \n",
    "\n",
    "We'll start with a two-layered network, utilizing a standard cross-entropy loss function. This is in contrast to the novel multi-label loss function: \"BP-MLL\" discussed in Zhang & Zhou (2006). We will fit an identical network architecture using BP-MLL later on, for comparison. Note, we're using relu and sigmoid activations, dropout regularization, and Adagrad optimization. This is due to intuitions from Nam et al. (2014) where results for networks using 'standard' losses performed as well, or better than BP-MLL when using similar designs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Att1</th>\n",
       "      <th>Att2</th>\n",
       "      <th>Att3</th>\n",
       "      <th>Att4</th>\n",
       "      <th>Att5</th>\n",
       "      <th>Att6</th>\n",
       "      <th>Att7</th>\n",
       "      <th>Att8</th>\n",
       "      <th>Att9</th>\n",
       "      <th>Att10</th>\n",
       "      <th>...</th>\n",
       "      <th>Class5</th>\n",
       "      <th>Class6</th>\n",
       "      <th>Class7</th>\n",
       "      <th>Class8</th>\n",
       "      <th>Class9</th>\n",
       "      <th>Class10</th>\n",
       "      <th>Class11</th>\n",
       "      <th>Class12</th>\n",
       "      <th>Class13</th>\n",
       "      <th>Class14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004168</td>\n",
       "      <td>-0.170975</td>\n",
       "      <td>-0.156748</td>\n",
       "      <td>-0.142151</td>\n",
       "      <td>0.058781</td>\n",
       "      <td>0.026851</td>\n",
       "      <td>0.197719</td>\n",
       "      <td>0.041850</td>\n",
       "      <td>0.066938</td>\n",
       "      <td>-0.056617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.103956</td>\n",
       "      <td>0.011879</td>\n",
       "      <td>-0.098986</td>\n",
       "      <td>-0.054501</td>\n",
       "      <td>-0.007970</td>\n",
       "      <td>0.049113</td>\n",
       "      <td>-0.030580</td>\n",
       "      <td>-0.077933</td>\n",
       "      <td>-0.080529</td>\n",
       "      <td>-0.016267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.509949</td>\n",
       "      <td>0.401709</td>\n",
       "      <td>0.293799</td>\n",
       "      <td>0.087714</td>\n",
       "      <td>0.011686</td>\n",
       "      <td>-0.006411</td>\n",
       "      <td>-0.006255</td>\n",
       "      <td>0.013646</td>\n",
       "      <td>-0.040666</td>\n",
       "      <td>-0.024447</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.119092</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>-0.002262</td>\n",
       "      <td>0.072254</td>\n",
       "      <td>0.044512</td>\n",
       "      <td>-0.051467</td>\n",
       "      <td>0.074686</td>\n",
       "      <td>-0.007670</td>\n",
       "      <td>0.079438</td>\n",
       "      <td>0.062184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042037</td>\n",
       "      <td>0.007054</td>\n",
       "      <td>-0.069483</td>\n",
       "      <td>0.081015</td>\n",
       "      <td>-0.048207</td>\n",
       "      <td>0.089446</td>\n",
       "      <td>-0.004947</td>\n",
       "      <td>0.064456</td>\n",
       "      <td>-0.133387</td>\n",
       "      <td>0.068878</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Att1      Att2      Att3      Att4      Att5      Att6      Att7  \\\n",
       "0  0.004168 -0.170975 -0.156748 -0.142151  0.058781  0.026851  0.197719   \n",
       "1 -0.103956  0.011879 -0.098986 -0.054501 -0.007970  0.049113 -0.030580   \n",
       "2  0.509949  0.401709  0.293799  0.087714  0.011686 -0.006411 -0.006255   \n",
       "3  0.119092  0.004412 -0.002262  0.072254  0.044512 -0.051467  0.074686   \n",
       "4  0.042037  0.007054 -0.069483  0.081015 -0.048207  0.089446 -0.004947   \n",
       "\n",
       "       Att8      Att9     Att10  ...  Class5  Class6  Class7  Class8  Class9  \\\n",
       "0  0.041850  0.066938 -0.056617  ...     0.0     0.0     1.0     1.0     0.0   \n",
       "1 -0.077933 -0.080529 -0.016267  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "2  0.013646 -0.040666 -0.024447  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "3 -0.007670  0.079438  0.062184  ...     0.0     0.0     0.0     0.0     0.0   \n",
       "4  0.064456 -0.133387  0.068878  ...     1.0     1.0     0.0     0.0     0.0   \n",
       "\n",
       "   Class10  Class11  Class12  Class13  Class14  \n",
       "0      0.0      0.0      1.0      1.0      0.0  \n",
       "1      0.0      0.0      0.0      0.0      0.0  \n",
       "2      0.0      0.0      1.0      1.0      0.0  \n",
       "3      0.0      0.0      0.0      0.0      0.0  \n",
       "4      0.0      0.0      0.0      0.0      0.0  \n",
       "\n",
       "[5 rows x 117 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the test data\n",
    "data_test = arff.loadarff('Yeast/Yeast-test.arff')\n",
    "df_test = pd.DataFrame(data_test[0])\n",
    "df_test[label_names] = df_test[label_names].astype(\"float\")\n",
    "X_test = df_test[np.setdiff1d(df_test.columns, label_names)].to_numpy()\n",
    "Y_test = df_test[label_names].to_numpy()\n",
    "\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start with standard cross-entropy loss (bpmll used later)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(14, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "optim_func = tf.keras.optimizers.Adagrad(\n",
    "    learning_rate = 0.001, initial_accumulator_value = 0.1, epsilon = 1e-07,\n",
    "    name = 'Adagrad')\n",
    "\n",
    "metric = tfa.metrics.HammingLoss(mode = 'multilabel', threshold = 0.5)\n",
    "\n",
    "model.compile(optimizer = optim_func,\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = metric\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "47/47 - 2s - loss: 0.6921 - hamming_loss: 0.4819 - val_loss: 0.6920 - val_hamming_loss: 0.4812\n",
      "Epoch 2/20\n",
      "47/47 - 0s - loss: 0.6910 - hamming_loss: 0.4690 - val_loss: 0.6906 - val_hamming_loss: 0.4688\n",
      "Epoch 3/20\n",
      "47/47 - 0s - loss: 0.6895 - hamming_loss: 0.4556 - val_loss: 0.6893 - val_hamming_loss: 0.4572\n",
      "Epoch 4/20\n",
      "47/47 - 0s - loss: 0.6884 - hamming_loss: 0.4471 - val_loss: 0.6881 - val_hamming_loss: 0.4464\n",
      "Epoch 5/20\n",
      "47/47 - 0s - loss: 0.6868 - hamming_loss: 0.4400 - val_loss: 0.6869 - val_hamming_loss: 0.4342\n",
      "Epoch 6/20\n",
      "47/47 - 0s - loss: 0.6860 - hamming_loss: 0.4284 - val_loss: 0.6858 - val_hamming_loss: 0.4237\n",
      "Epoch 7/20\n",
      "47/47 - 0s - loss: 0.6846 - hamming_loss: 0.4166 - val_loss: 0.6847 - val_hamming_loss: 0.4138\n",
      "Epoch 8/20\n",
      "47/47 - 0s - loss: 0.6838 - hamming_loss: 0.4123 - val_loss: 0.6837 - val_hamming_loss: 0.4043\n",
      "Epoch 9/20\n",
      "47/47 - 0s - loss: 0.6828 - hamming_loss: 0.4032 - val_loss: 0.6826 - val_hamming_loss: 0.3959\n",
      "Epoch 10/20\n",
      "47/47 - 0s - loss: 0.6815 - hamming_loss: 0.3918 - val_loss: 0.6816 - val_hamming_loss: 0.3877\n",
      "Epoch 11/20\n",
      "47/47 - 0s - loss: 0.6804 - hamming_loss: 0.3872 - val_loss: 0.6807 - val_hamming_loss: 0.3801\n",
      "Epoch 12/20\n",
      "47/47 - 0s - loss: 0.6792 - hamming_loss: 0.3769 - val_loss: 0.6797 - val_hamming_loss: 0.3725\n",
      "Epoch 13/20\n",
      "47/47 - 0s - loss: 0.6786 - hamming_loss: 0.3744 - val_loss: 0.6788 - val_hamming_loss: 0.3654\n",
      "Epoch 14/20\n",
      "47/47 - 0s - loss: 0.6775 - hamming_loss: 0.3647 - val_loss: 0.6779 - val_hamming_loss: 0.3577\n",
      "Epoch 15/20\n",
      "47/47 - 0s - loss: 0.6770 - hamming_loss: 0.3642 - val_loss: 0.6770 - val_hamming_loss: 0.3519\n",
      "Epoch 16/20\n",
      "47/47 - 0s - loss: 0.6764 - hamming_loss: 0.3615 - val_loss: 0.6761 - val_hamming_loss: 0.3458\n",
      "Epoch 17/20\n",
      "47/47 - 0s - loss: 0.6751 - hamming_loss: 0.3515 - val_loss: 0.6752 - val_hamming_loss: 0.3408\n",
      "Epoch 18/20\n",
      "47/47 - 0s - loss: 0.6743 - hamming_loss: 0.3467 - val_loss: 0.6743 - val_hamming_loss: 0.3364\n",
      "Epoch 19/20\n",
      "47/47 - 0s - loss: 0.6735 - hamming_loss: 0.3408 - val_loss: 0.6735 - val_hamming_loss: 0.3310\n",
      "Epoch 20/20\n",
      "47/47 - 0s - loss: 0.6725 - hamming_loss: 0.3386 - val_loss: 0.6727 - val_hamming_loss: 0.3256\n"
     ]
    }
   ],
   "source": [
    "history_ce = model.fit(X_train, Y_train, epochs = 20,\n",
    "                       validation_data = (X_test, Y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Test Set Performance Using Constant and Learned Threshold Functions\n",
    "\n",
    "Next, let's compare how test-set performance is affected when a constant threshold function is swapped out for a learned threshold function. After generating logit predictions, we'll first apply a constant threshold function ($t(x) \\equiv 0.5$ -- the default) to generate binary predictions. Using hamming loss as a metric, we'll then evaluate how our model performed on the test set. In fact, we can just grab this value from the model's training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3255959451198578"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_ce = history_ce.history['val_hamming_loss'][19]\n",
    "constant_ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a constant threshold value of $0.5$ yields a validation hamming loss of $\\approx 0.33$. Next, we'll utilize the `predict_test_labels_binary()` function from the `threshold_learning` library to generate binary predictions from a learned threshold function. We find that the hamming loss can be significantly reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30222776133354107"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Learn a Threshold Function\n",
    "Y_train_pred = model.predict(X_train)\n",
    "Y_test_pred = model.predict(X_test)\n",
    "t_range = (0, 1)\n",
    "from threshold_learning import predict_test_labels_binary\n",
    "\n",
    "test_labels_binary, threshold_function = predict_test_labels_binary(Y_train_pred, Y_train, Y_test_pred, t_range)\n",
    "learnedTF_ce = metrics.hamming_loss(Y_test, test_labels_binary)\n",
    "learnedTF_ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Train a Network Using BP-MLL Loss\n",
    "\n",
    "Using the same architecture as in the previous example, we'll now learn a network using the BP-MLL loss function as described in Zhang & Zhao (2006). This loss function aims to increase model performance by minimizing pairwise errors. Namely, let $c_j^i$ denote the output of the label $j$ node of the  network for instance $i$. Furthermore, let $Y_i$ denote the label set of instance $i$ and $\\overline{Y}_i$ denote it's complement, in the set of possible labels. Then, the BP-MLL loss is given by:\n",
    "\n",
    "$$\n",
    "    E = \\sum_{i = 1}^m E_i = \\sum_{i = 1}^m \\frac{1}{|Y_i| |\\overline{Y}_i|} \\sum_{(k,l) \\in Y_i \\times \\overline{Y}_i} \\exp(-(c_k^i - c_l^i))\n",
    "$$\n",
    "\n",
    "so that the $i^{th}$ error term is severely penalized if $c_k^i$ is much smaller than $c_l^i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start with bp-mll loss\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(14, activation = 'sigmoid')\n",
    "])\n",
    "\n",
    "optim_func = tf.keras.optimizers.Adagrad(\n",
    "    learning_rate = 0.001, initial_accumulator_value = 0.1, epsilon = 1e-07,\n",
    "    name = 'Adagrad')\n",
    "\n",
    "model.compile(optimizer = optim_func,\n",
    "              loss = bp_mll_loss,\n",
    "              metrics = metric\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "47/47 - 1s - loss: 0.9997 - hamming_loss: 0.4238 - val_loss: 0.9987 - val_hamming_loss: 0.4695\n",
      "Epoch 2/20\n",
      "47/47 - 0s - loss: 0.9989 - hamming_loss: 0.4763 - val_loss: 0.9977 - val_hamming_loss: 0.4639\n",
      "Epoch 3/20\n",
      "47/47 - 0s - loss: 0.9976 - hamming_loss: 0.4682 - val_loss: 0.9967 - val_hamming_loss: 0.4564\n",
      "Epoch 4/20\n",
      "47/47 - 0s - loss: 0.9972 - hamming_loss: 0.4625 - val_loss: 0.9958 - val_hamming_loss: 0.4494\n",
      "Epoch 5/20\n",
      "47/47 - 0s - loss: 0.9962 - hamming_loss: 0.4583 - val_loss: 0.9949 - val_hamming_loss: 0.4442\n",
      "Epoch 6/20\n",
      "47/47 - 0s - loss: 0.9953 - hamming_loss: 0.4512 - val_loss: 0.9941 - val_hamming_loss: 0.4378\n",
      "Epoch 7/20\n",
      "47/47 - 0s - loss: 0.9941 - hamming_loss: 0.4467 - val_loss: 0.9932 - val_hamming_loss: 0.4315\n",
      "Epoch 8/20\n",
      "47/47 - 0s - loss: 0.9937 - hamming_loss: 0.4408 - val_loss: 0.9924 - val_hamming_loss: 0.4267\n",
      "Epoch 9/20\n",
      "47/47 - 0s - loss: 0.9928 - hamming_loss: 0.4325 - val_loss: 0.9917 - val_hamming_loss: 0.4213\n",
      "Epoch 10/20\n",
      "47/47 - 0s - loss: 0.9919 - hamming_loss: 0.4310 - val_loss: 0.9909 - val_hamming_loss: 0.4166\n",
      "Epoch 11/20\n",
      "47/47 - 0s - loss: 0.9909 - hamming_loss: 0.4266 - val_loss: 0.9902 - val_hamming_loss: 0.4116\n",
      "Epoch 12/20\n",
      "47/47 - 0s - loss: 0.9907 - hamming_loss: 0.4230 - val_loss: 0.9894 - val_hamming_loss: 0.4053\n",
      "Epoch 13/20\n",
      "47/47 - 0s - loss: 0.9898 - hamming_loss: 0.4171 - val_loss: 0.9887 - val_hamming_loss: 0.4013\n",
      "Epoch 14/20\n",
      "47/47 - 0s - loss: 0.9888 - hamming_loss: 0.4125 - val_loss: 0.9880 - val_hamming_loss: 0.3978\n",
      "Epoch 15/20\n",
      "47/47 - 0s - loss: 0.9881 - hamming_loss: 0.4073 - val_loss: 0.9873 - val_hamming_loss: 0.3946\n",
      "Epoch 16/20\n",
      "47/47 - 0s - loss: 0.9876 - hamming_loss: 0.4046 - val_loss: 0.9866 - val_hamming_loss: 0.3906\n",
      "Epoch 17/20\n",
      "47/47 - 0s - loss: 0.9868 - hamming_loss: 0.4005 - val_loss: 0.9859 - val_hamming_loss: 0.3869\n",
      "Epoch 18/20\n",
      "47/47 - 0s - loss: 0.9860 - hamming_loss: 0.4001 - val_loss: 0.9853 - val_hamming_loss: 0.3822\n",
      "Epoch 19/20\n",
      "47/47 - 0s - loss: 0.9857 - hamming_loss: 0.3933 - val_loss: 0.9846 - val_hamming_loss: 0.3796\n",
      "Epoch 20/20\n",
      "47/47 - 0s - loss: 0.9852 - hamming_loss: 0.3923 - val_loss: 0.9839 - val_hamming_loss: 0.3764\n"
     ]
    }
   ],
   "source": [
    "history_bpmll = model.fit(X_train, Y_train, epochs = 20,\n",
    "                          validation_data = (X_test, Y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Test Set Performance Using Constant and Learned Threshold Functions\n",
    "\n",
    "Again, we'll grab the hamming loss on our test data from the model's training history and compare it with the loss when using a learned threshold function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37638258934020996"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_bpmll = history_bpmll.history['val_hamming_loss'][19]\n",
    "constant_bpmll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31944228072908554"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Learn a Threshold Function\n",
    "Y_train_pred = model.predict(X_train)\n",
    "Y_test_pred = model.predict(X_test)\n",
    "t_range = (0, 1)\n",
    "from threshold_learning import predict_test_labels_binary\n",
    "\n",
    "test_labels_binary, threshold_function = predict_test_labels_binary(Y_train_pred, Y_train, Y_test_pred, t_range)\n",
    "learnedTF_bpmll = metrics.hamming_loss(Y_test, test_labels_binary)\n",
    "learnedTF_bpmll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'BPMLL vs Cross Entropy Training History')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEbCAYAAAArhqjIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABLVElEQVR4nO3dd3hUZfbA8e8hJITeeygBQZqCdBAVRaWIIiLNBpYfi7u69lVX3dXVXcu61rWxFrDSxAIiggoq0kFa6J3Qew+knN8f7w0OQ8okmZJyPs8zDzN3bjm5c5kzb7nvK6qKMcYYk5VikQ7AGGNM/mfJwhhjTLYsWRhjjMmWJQtjjDHZsmRhjDEmW5YsjDHGZMuSRSZE5EkRUZ/HcRFZJiLD/Nar77feERFZICIDMlnnwgyO9YT33qYMtumdRYwzRGR8kP7kPBORhiLynohsFZFTIrJHRMaLSMdIx5YZv8/O97Euh/uJ8a6ZViEKNWgyuGYze9TP5f67etu3yOF2Q73tyuTmuLnhHe+uDJaf9f9PRDaJyIs52PcwEbk2SKFGXPFIB5DPHQJ6eM9LA1cD74jIUVX91G/dB4FfgXLArcAYETmuqpN81jkKDPbW8zXQe6/A8pLgZGAt8DdgPVAVuA74VUQqqeqhCIaYlf8A/kk3KYf7iAH+DmwCFuc9pJDaAXTyed0A+AT4E7DIb73cWOTtf30Ot/vG2+54Lo8ban2BfTlYfxiwHPgyJNGEmSWLrKWo6hyf1z+ISGfgWsA/WaxOX1dEvgdaA3cCvsliInC9iNyjqqneuucBTYGxnPkfuMAQkZLAGGA+0EtVT/m8/bmIvAskZ7JtNJCWfj4iZJPf5xxSIlJSVU+E63j+VPUkcPrvFZH0HyorMjsPOfmcVPWw7/5zENceYE9OtwsXVf0tUscWkSggyu//VlhZNVTOHQGis1pBVdNwvy7r+731FVAWuNRn2SBgJrAtaBFmQkSeEpGdIlLMb3lvr8h9jvf6GhFZKCLHROSAiMwVkUuy2HV/oDZwX0YXs6pOV9Xj3r5neFVTw0RkPe4XfC0RifKqcbaIyEkRSRCRG/zibC4iU0RkvxfbShH5k8/7XUTkFxE57D0Wi0j/XJ+w3/c70qtavEJElnrHnikizX1WO+L9+4FvNY5PdcaNIvKhiBzE/WhAROJF5Esv1iMiMjH9M/A5torI/SLyqvd3HxSR10Ukxnu/kogkicgQv+1ERDaKyEu5/Jsz+5yaiMhocVWNx73P6V7fayqjaijv9T0i8i9x1ZO7ReQNESnhs84Z1VA+526AiLwjIodEJNG7jv2v4f4islZETojIdBG5wNt2aG7+/kzOyRnVUFldjyIyA2gDDPG5HoZ67wVyradfc9eKSIJ3/juF4rMOlJUssiEi6eeoFHANcAlwWwCb1gd2+i07hitpDAa+95YNAv4NnEPojcZVEV0CTPdZPgBYqKrrRKQhrkrmVeAhIBZ30VfKYr+XANtVdVmAcVwINAQexlU5HAL+AfwFeApXQukHfCIiqqqfedt9DawCbgJOAufiqv0QkXK4c/uVty8BzgMqBBBPMZ/POV2al/TT1cV9Tv8ETgAvAmNFpIW6MXMuA34EnsFVp4CrxqnpPX8RmIBLrKnel+QPuBLX/wEp3t/+k4icp6r7fY79AO6X+o1Acy+GJOAhVd0vIl/gqj5H+WzTFXcNfhDA35+ZjD6nxsBqXLXVEaCVF3dJ4Nls9vcA7hzdBJzvrb8ZeCGb7V4APgeuB7rhruEEXGkcEWmLu7bHA3fjSupjAv0jyfjzjwpgu0yvR+CPXswbgKe9ZenVcoFc6+A+vxe89Xd524fqs86eqtojgwfwJKAZPF71W6++t/waXPKthLsQFLjLb53euHrP/bg67va4L4squC+TTRnst3cWMc4Axufw71oCvO3zugTuS+BB7/X1wL4c7nMKMDvAdWfgvmxr+CyrhEukf/dbdzKueg/vHClwXib7beu9XzaHsWf0GSsw0medkbgv80Y+y6711mvivS7jvR6ayfXxhd/y4d4+G/gsiwNOAY/6xbcKKOaz7DHcl3cl7/XlQJrfvj4EFgR4Dlp4x+ma1eeUwXbiXfN/BTb4LO/q7a+F39/xs9/2XwJzfF4P9dYr43fuPvTbbjEw2uf1OFzbgPgsS/8/ODSbvz2zzz/90dtn3U3Ai4Fcj946C3yvo0CvdZ9rToFWfuvl6bPOy8OqobJ2CGjnPboA9+CKlX/PYN2vcF/8+3C/Ll8C3spgvcm4Xy3dcaWKH1R1b/BDz9QYoJ/PL6meuKqxsd7rZUB5ERklIleKSOkA95uTESkXqqpvqasFruQ2LoNYG4tINVyC3Qq8LSIDvWW+1uM6CXwqIn1EpEIO4vk3v3/O6Y8n/dbZpKprfV6v8P6NC/AY3/i9bg8sUtUN6QtUNRHX+aGL37pf6ZmlnAm4X/Lp1Tw/4H6hDwEQkbK4jgV5/aXp/zkhIrFeNdA63K/pZFxJJz6DX+f+pvq9XkFg5y+77doBE9X75vR8HcB+02X0+V+TzTbZXY+ZCeRaT7dNVRf7rReqzzpbliyylqKqC7zHr6r6Gq5I+VcR8a+WuQ93kTXB/TJ6QDNoDFTXuPglcAOu+md0SP+Cs43G/Sq6zHs9EFcq2OLFtxrog+shMxnYKyKfikjVLPa5DVdNE6hdfq9rZrI8/XVF78vySlzV3vvATnHtExd4cR/w3o/GJb49IvKNiDQIIJ4tPp9z+mOT3zoH/V6nt83EBrB/378lXc0MlqWv539t7c7kdU0A70vyA9wPGcFdV8U5uxNGTmUU3/O4nn8jgF64a/4Z773szsVBv9enAtgmkO1qcHbDeE4ays/6/HE/mjKV3fWYhWyv9QyW+R43VJ91tixZ5NwKXBVSQ7/l67wLbbVm32NhNO5DroKrgwwb75fsAmCgiJTCdQce47fON6p6EVAZuB1X9H09i93OAGrLmQ2+WYbh9zq9i6b/r7Pq3r/7vbhWqWo/XDvE5bgvjG/SGztVdbaq9vDevw5Xvx7y/0QByuhvzujXaHW8v9eH/3rpr327tn4A1MF1nhgKfOkl0LzIqLTYH3hdVV9Q1e+9L9aUPB4nr3biumn7yurHTVBkdz1mIqBrPf0QmewjFJ91tixZ5Fx60X9rHvYxDdf49YJG5t6D0bi2k7646gz/IjEAqnpI3f0kXwDNstjfeFzp4mVxXSzP4PWOKZXF9stxdfD+PZcGAGvUdan0jStZVX/EVfXVxK8RW1VPqOpE3C++rOIOppyWNOYCbUQkPn2BiNQGOuN6x/nq4/cFdB2uPWF5+gJV3YqrrnkKV40VqmqJkrjqp/SYo3DVqZE0H7ja+6WdLrtqpKDJ4nrMqOSUo2s9k+OF67M+g/WGylpx+f3u4xhcr6DHcXXI/j2dAqaqKbiLIxAXioj/BbfJ+0UH7hf99RkcI6s7u8fi6mn/jWt0PP0LVUT+gLvfYwqwHWiEu7A/zGxnqnpCRAYC3+JuwHsD1wukCq4h+EZcKSWz7feLyCvA4yKSgiv5XIer5hjsxXU+rhPAGG/fFXG9dJZ421+F66X2JbAF15X3D7jeN9mpL2ffZa6qOjeAbdNXPiUiG4EBIrIc11tpaRabjPTi/1ZE/gak4tpJ9gLv+K1bFhgnIv/D9Yb6G/BfPbPHFMB7uMSfiPtBEgrTgD95bRb7cTfylch6k5B7Hpd8R4vIB7jeUP/nvZeW6VZ5kN316K22CuguIt1xbZkbVXVfdtd6gMLxWZ/BkkXWygOzvefJuIalt/m9jjYcHslg2Shc8ROgIxmXDCSDZYD7ZSIis3BdI5/ye3sp7lfZS7i68x3A/3BfUJlS1V9FpDWuZ8w/ccXqg7hfyVcEUIL6G646405v23XATaqa3qazE1eH+xhQy9v3dNx/ULz1FfgXroi/B9eV9q/ZHBdcl84H/JalkvP/H8NxXyDf475A4zNbUVVPisjluPP8Hu7zmgFcl0ES+A+uDekzXG3Au2T8d03CncNRfg3iwXQ37v/AG7jSzShcyXNEiI6XLVVdICKDcZ99H9wX8J24L9HDITpsdtcjuO+JurgfZ+kjO4wk+2s9EOH4rM8gZ3YgMMbkJyKiwN2q+t8A1u2F+xJprKo5GtuqsBGRm4CPcF1MN0Y6nmCLxGdtJQtjCjgRqYWrLnwOmFwUE4WIvIUrSRzADbXzOPBNYUsUkfysrYHbmIJvGK7/fRKumqgoqgy8iWv4fQjXlnBDllsUTBH7rK0ayhhjTLasZGGMMSZbhbbNokqVKlq/fv1Ih2GMMQXGwoUL96pqhjc0FtpkUb9+fRYsWJD9isYYYwAQkc2ZvWfVUMYYY7JlycIYY0y2LFkYY4zJVqFtszDGGF/JyckkJiaSlJQU6VAiLjY2lri4OKKjs5wh+gyWLIwxRUJiYiJly5alfv36nDlAbdGiquzbt4/ExETi4zMdvuwsVg1ljCkSkpKSqFy5cpFOFAAiQuXKlXNcwrJkYYwpMop6okiXm/MQtmQhIj1EZLWIrBORjIbdTl+vnYik+s7RICL3iUiCiCwXkc8ymN8hOFThp3/DjqymITDGmKInLMnCm03rDaAnbuaywSJy1gxm3nrPA9/5LKsN/Bloq6otgNDNzHXiACwcCaOuhu2/heQQxpiiKyoqilatWtGyZUtat27NrFmzANi0aRMlS5akVatWNGvWjOHDh5OWlsamTZsQEZ544onT+9i7dy/R0dHcddddADz55JO8+OKLZx2rTJkyQY09XCWL9rg5qjd481OPxk1S4u9u3HSj/hPUFwdKikhxoBRuBrfgK1UJbv0GYsvBqD6QuDAkhzHGFE0lS5Zk8eLFLFmyhGeffZZHH3309HsNGzZk8eLFLF26lBUrVvDll18C0KBBAyZNmnR6vXHjxtG8eaDT3QdPuJJFbc6cszrRW3aaV4Loi5uF6zRV3YabfWwLbta2Q6o6NaODiMgwEVkgIgv27Ml2KtuMVawPQ7+BUhXho2th67zc7ccYY7Jw+PBhKlaseNby4sWL07lzZ9atc1NVlCxZkqZNm54evmjMmDEMGBDorMzBE66usxm1pviPjf4K8LCqpvo2vohIRVwpJB43deE4EblJVT8+a4eqI/Cmd2zbtm3ux16vUBeGToZRveGjvnDjeKjXKde7M8bkL09NTGDF9uDOuNqsVjn+fnXWv/hPnDhBq1atSEpKYseOHfz449lTxB8/fpwffviBf/zjH6eXDRo0iNGjR1OjRg2ioqKoVasW27eHpoIlM+FKFolAHZ/XcZxdldQWN+E6QBWglzeheTRuovM9ACIyAegMnJUsgqp8bS9hXA0f94Mbx0L9LiE9pDGmcEuvhgKYPXs2t9xyC8uXLwdg/fr1tGrVChGhT58+9OzZk02bNgHQo0cPnnjiCapXr87AgQMjEnu4ksV8oJGIxAPbcA3UZ8xipaqn7w4RkZHAJFX9UkQ6AB1FpBRugvhuuAnZQ69cTVclNepq+Ph6uGE0NOgalkMbY0InuxJAOHTq1Im9e/eSXmWe3maRkZiYGNq0acN//vMfEhISmDhxYhgjdcLSZqGqKcBduF5OK4GxqpogIsNFZHg2284FxgOLgGW4mEeEOOTfla3uEkalePh0IKz7IWyHNsYUXqtWrSI1NZXKlSsHtP4DDzzA888/H/D6wRa24T5UdTIw2W/Z25msO9Tv9d+Bv4csuOyUqQpDJsGHfeCzwTDwY2h8ZcTCMcYUTOltFuCG3Rg1ahRRUVEBbdu8efNMe0E988wzvPLKK6dfJyYmcvz4ceLi4k4vu//++7n//vtzHXuhnYO7bdu2GvTJj47vdz2kdq+EAR/CuT2Du39jTMisXLmSpk2bRjqMfCOj8yEiC1W1bUbr23AfOVGqEtzyFVRvAWNuhpXhrzc0xphIsGSRUyUrwi1fQq1WMG4oJHwZ2XiMMSYMLFnkRmx5uGkC1G4L42+DZeMjHZExxoSUJYvcii0HN30OdTvChP+DJWMiHZExxoSMJYu8KFEGbhznbtb74g/w2yeRjsgYY0LCkoWff36zgsnLdpCSmhbYBjGlYfAYd7PeV3+ChaNCGp8xxkSCJQsfR5KSmbpiF3/8ZBFdX5zBezM3ciQpOfsNY0rB4NFwTjeY+GeY97/QB2uMKXB27tzJoEGDaNiwIc2aNaNXr16sWbPm9PDk6Y8PP/ww0qGexebg9lE2NpofH+jKtBW7eG/mBp6etIJXpq1hUPs6DOlcn7iKpTLfODoWBn3qekhNftDdk3HJX8Bm5jLG4G7C69u3L0OGDGH06NEALF68mF27dmU51Ed+YcnCT1QxoUeLGvRoUYMlWw/y3syNvP/rJt7/dRM9W9Tgjosa0KpOhYw3Ll4CBnwEX98NM/4Fx/dBj+egmBXgjCnqpk+fTnR0NMOH/z7CUatWrU4PFpjfWbLIQss6FXht8AU80rMJo2Zt4tN5W5i0dAdt6lXkji7xXNm8BlHF/EoOUcWhzxvuBr7Z/4UT+6HPm1A8JjJ/hDHmbN8+AjuXBXefNc6Dns9l+vby5ctp06ZNhu+ljzib7vXXX+eiiy4Kbnx5ZMkiALUqlOTRXk25u1sjxi3Yyvu/buTOTxZRp1JJbu0cz4B2dShTwudUFisGVz4DpSrDD0+56VoHfOgaw40xxo9VQxUyZUoU59YL47mlU/3T7Rr/mLSCl6etYXCHugzpXJ/aFUq6lUXgovtdCWPSffDhtXDDGPfaGBNZWZQAQqV58+aMH19wb+C1yvRcSG/XGDe8M1/+6UK6NqnGezM3cvEL07nr00Ws2ukzA1ebodB/JOxYDCOvgsM7IhS1MSaSLrvsMk6ePMn//vd7b8n58+ezefPmCEYVOEsWedSqTgVeH3wBP//lUu7oEs9Pa/Zw1WszefLrBA6d8LrdNuvjpmY9uAXevxL2rY9s0MaYsBMRvvjiC6ZNm0bDhg1p3rw5Tz75JLVq1TrdZpH+eO211yId7llsiPIgO3DsFP+ZtppP526hYqkYHu7RhOvbxFGsmMC2RfDJ9SDF3FAhNVuGPT5jiiobovxM+XaIchHpISKrRWSdiDySxXrtRCRVRK73WVZBRMaLyCoRWSkincITdc5VLB3DM9eex9d3daF+ldL85fOl9H1rFksTD0Lt1nDrFIgqASN7w6ZfIx2uMcYEJKBkISKDRaSp9/xcEflZRH4UkSYBbh8FvAH0BJoBg0WkWSbrPY+bftXXq8AUVW0CtMRNzZqvtahdnvHDO/HSgJZsO3CCPm/8yqMTlrK/VH24/TsoWxM+vg5WTc52X8YYE2mBliyeAfZ7z18E5gE/A28GuH17YJ2qblDVU8BooE8G690NfA7sTl8gIuWAi4H3AFT1lKoeDPC4ESUiXNc6jukPXsLtF8YzbkEil744gw9XpJAyZDJUbw5jbrIBCI0Jk8Ja7Z5TuTkPgSaLqqq6S0RigS7AY8A/gFYBbl8b2OrzOtFbdpqI1Ab6Av7zcjcA9gAfiMhvIvKuiGR4w4KIDBORBSKyYM+ePQGGFnplY6N5vHczvr3nIlrULsffvkrg6vdXsrDrKIi/GL76I8x6PdJhGlOoxcbGsm/fviKfMFSVffv2ERsbm6PtAr3PYo+InAOcB8xX1ZMiUgoIdOCjjNbz/8ReAR5W1VQ5czyl4kBr4G5VnSsirwKPAE+ctUPVEcAIcA3cAcYWNo2ql+Xj2zvw7fKdPDNpBf3eW8r1Lf/KM43LEjv1cTc8SLe/23hSxoRAXFwciYmJ5KcfkpESGxtLXFxcjrYJNFk8DSwEUoGB3rJuwJIAt08E6vi8jgO2+63TFhjtJYoqQC8RSQHmAImqOtdbbzwuWRRIIkKv82rS9dyqvDl9PSN+3sB3UYMZExdNs5kvu4TR+xUoFhXpUI0pVKKjo4mPj490GAVWQNVQqjoSqAnEqeo0b/FcYFCAx5kPNBKReBGJ8bb72u8Y8apaX1Xr4xLCH1X1S1XdCWwVkXO9VbsBKwI8br5VKqY4D3Y/l6n3XUz7BlXpteE6PooZAIs+hDE3w6ljkQ7RGGNOC6hkISJVgROqetTrsXQLrpTxcSDbq2qKiNyF6+UUBbyvqgkiMtx737+dwt/dwCdeotkA3BrIcQuC+lVK897Qdvywchf/mFSatcdK8uTqjzj1vx7E3jIOytaIdIjGGBPYTXkiMhcYrqq/ichzwNVAMjBdVe8LcYy5Eqmb8vIiKTmV92ZuZMWMsbzAKyTHlCdt8BgqNWgd6dCMMUVAVjflBZosDgCVVFVFJBHoDBwFElS1ZlCjDZKCmCzS7T16krETv+G6VfdThiR+aPE8V/S5kVIxNu6jMSZ0gnEHdyoQIyLnAYdUdQtwECgTnBCNryplSvDHwddxcuhU9peozVXL7+W15x5l9LwtpKblu05expgiINBk8S0wFngLd0MduDuxt4UiKOPUi29M3QdmcLROVx5JG8Hhrx+h1yszmL5qd5HvK26MCa9A6zXuAIbg2ik+8pZVAZ4MQUzGV4myVLh1HDrlEYbN/x9Nju1j2Mg/0LphLf7aqyktapePdITGmCIgR6POikgxoDqwS1XTQhZVEBTkNotMzXkbnfII+8o1ZdCRe1l3ogx9L6jNA1c2Jq5iqUhHZ4wp4PLcZiEi5URkFHACV/V0QkRGiYj9rA2njsORQZ9S5cQmppb9B39rD5OX7eCy//zEs5NXcuh4cqQjNMYUUoG2WbyGa8w+Dyjp/VvKW27CqUkvuHUyxdJSuG31cGb1h6vPr8WIXzZwyYvTefeXDZxMSY10lMaYQibQrrM7gQaqetxnWRlgvapWD2F8uVYoq6F8HUqETwfC7pXQ+yVW1LyOZ79dyS9r91KlTAwD29VhcPu6Vj1ljAlYMLrOJgFV/ZZVAU7mJTCTB+Xj4LYp0PAymHgPzZb/m49ubcend3TggroVeWvGei5+YTp3jFrAT2v2kGZdbo0xeRBob6h3gWki8hKwGagH3Af8L8utTGiVKAuDR8O3f4FZr8GBTXTu+w6dz2nLtoMn+GzuFkbP38L3K3dRr3IpbuxQl/5t6lCxdEykIzfGFDCBVkMJbjymG4BauBFjP1PV90IbXu4V+mooX6ow50347jE3devg0VCmGgCnUtKYkrCTj2dvZt6m/cQUL8bV59fi5k71aBlXHrHh0I0xnjwP95HJTovjBgS8JS/BhUqRShbpVk6CCf/npmwdMhHKnzG/FKt2HubjOZv5YtE2jp1K5bza5bm5Yz2ublmLkjE2JLoxRV2okkUJ4Liq5stvmSKZLAC2zIVProeSFV3CqFjvrFWOJCXz5W/b+GjOZtbsOkq52OL0b1uHGzvUpUFVG8HFmKLKkkVRs20hfNQXYsrCkK+hcsMMV1NV5m3cz0dzNjNl+U5S0pQu51Th9ovi6dq4qlVRGVPEWLIoinYshQ/7QFSMK2FUbZzl6ruPJDFm3lY+mbuFnYeTaFWnAvdf0ZiLGlWxpGFMEZHrZCEit2Wx32jgTUsW+diuFS5hoHDLV1C9ebabnEpJY/zCRP7741q2H0qiTb2K3H9FYzo3rGxJw5hCLi/JYnp2O1fVSwMMogfwKm6mvHdV9blM1muHm3d7oKqO91keBSwAtqlq7+yOZ8nCs3ctjLoGUk7AzV9CrVYBbXYyJZWx87fyxvT17DycRPv4Stx/RWM6Nqgc0nCNMZETkmqoHAYQBawBrgAScXNyD1bVFRmsNw13E+D7fsnifqAtUM6SRQ7t3+gSRtIhuHkCxGV4LWQoKTmV0fO28MaM9ew5cpJODSpz3xWNaR9fKYQBG2MiIRh3cOdVe2Cdqm5Q1VO4OTH6ZLDe3cDnwG7fhSISB1yFuznQ5FSleLj1GyhVyVVLbZ4V8Kax0VEMvTCeX/5yKU/0bsba3UcZ8M5sbnp3Lgs3Hwhh0MaY/CRcyaI2sNXndaK37DQRqQ30Bd7OYPtXgL8A+XpY9HytQl24dbK7B+PjfrDhpxxtHhsdxe1dXNJ4rFdTVu44TL+3ZjHk/Xks3nowNDEbY/KNcCWLjFpG/eu/XgEeVtUzhkwVkd7AblVdmO1BRIaJyAIRWbBnz55cB1tolavlEkbF+vDpAFj7fY53UTImiv+7uAG/PHwpj/RswtLEg1z7xq/cNnI+yxIPBT9mY0y+EK42i07Ak6ra3Xv9KICqPuuzzkZ+TypVgOPAMKADcDOQAsQC5YAJqnpTVse0NossHNsHH/WBPauh/yg37HkuHT2ZwqhZmxjx8wYOnUjm8qbVuffyRjaDnzEFUJ4buEWkQSZvnQR2ZDdrnjc0yBqgG27ypPnADaqakMn6I4FJvg3c3vKuwIPWwB0EJw7AR9fBzqXQ7z1ofm2ednckKZkPft3Eu79s4HBSClc2q849lzeieS1LGsYUFMFo4F4HrPUevs+3ACdF5HMRyXReC1VNAe4CvgNWAmNVNUFEhovI8MD/FBM0JSu6ey9qt4Xxt8LSsXnaXdnYaP7crRG/PHwZ913emNkb9nHVazMZ9uECErZb9ZQxBV2gJYvbgUuAp3AN1XWBJ4BZwE/A80Cyql4fulBzxkoWATp5FD4bBJtmwjWvQ+ubg7LbQyeS+eDXjbw3cyNHrKRhTIEQjGqoROAcVU3yWVYKWKOqcSJSEVirqlWCFXReWbLIgVPHYcyNsP5HuOolaHd70HZtScOYgiMY1VDFgPp+y+ri7sYGOErgEymZ/CamFAz6DBr3gG/uh5kvuzkygqB8yWjuvbwxMx++jHsvb3S6euoPH1n1lDEFSaBf8K8AP4rIB7hqqDjcZEiveO9fBcwOdnAmjKJjYcBH8MUf4PsnYc8auPoVKF4iKLtPTxq3Xhh/uqTxXcIuujevzp+7WUnDmPwu4K6z3thO/XEz5e3ANVJPCWFseWLVULmUlgY/vwAznoW49jDok9Oz7gWTf/WUJQ1jIi/iY0NFgiWLPEr4Ar64E0pVhsGfQc3zQ3KYjJLGHy5pyAV1Ktgot8aEWTAauGOAoUAr4Iyp1Gxa1UJs+2IYfYO7J6PvO9DsmpAdyj9pNKpWhgFt69C3dW2qlAlOVZgxJmvBSBafAS2Bibg7q09T1aeCEWSwWbIIkiO7XE+pxPlw6WNw8UMQwl/8R5KSmbR0B2MXbOW3LQcpXky4rEk1BrarwyWNq1I8Klwj1BhT9AQjWRwA4lX1YJBjCxlLFkGUnAQT74Glo6H5ddDnDdeDKsTW7jrCuIWJTFiUyN6jp6hatgT9WsfRv20cDW2ucGOCLhjJYglwparuCnZwoWLJIshU4ddXXU+pmi1dO0a5WmE5dHJqGtNX7WbsgkSmr95NaprStl5FBrStQ6/za1KmhPXaNiYYgpEsHsD1hHoVOCNhqOqPwQgy2CxZhMjqb+HzOyCmDAz6FOLahPXwu48k8cWibYxZsJUNe45RKiaKq86ryYB2dWhbr6I1ihuTB8FIFhszeUtVNbNBBiPKkkUI7Vrhhgg5stNVSZ3fP+whqCqLthxk7PytTFq6nWOnUomvUpr+beO4vk0c1crGhj0mYwo66zprgu/YPhh7C2yeCRc9AJc+DsUi0/h87GQKk5ftYNyCROZt2k90lNCzRU2GdK5P67rWBdeYQFmyMKGRcgomPwiLRsG5V8F1I6BEZBueN+w5ysdztjBuwVaOnEzhvNrlGdK5Pr3Pr0lsdFT2OzCmCMtVshCRlara1Hu+lbNntgNAVesGK9BgsmQRJqowbwRMeQSqNXPtGBXrRToqjp1MYcKiREbN3sy63UepXDqGQe3rcFPHetQsXzLS4RmTL+U2WXRR1Zne80sy27mq5mwy5zCxZBFm63+EcUOhWHEY+DHU6xzpiADXtvHrun2MnLWJH1btopgI3ZtXZ2jneNrVtwZxY3xZNZQJj73r4LOBcGATdP8XtB8W0hv4cmrr/uN8NGczY+Zv5dCJZJrWLMfQzvXo06q2VVEZQ3B6QxUHBgMXcPZwH8OCEWSwWbKIkKRD8MVwWD0Zzh8EvV8Oyw18OXHiVCpfLt7GqFmbWLXzCBVKRTOwXR1u7liPuIr5K1ZjwikYyWI0cB7wLXDC9z1VfSLAIHrg7tOIAt5V1ecyWa8dMAcYqKrjRaQO8CFQA0gDRqjqq9kdz5JFBKWlwS//gen/hBotXLVUxfqRjuosqsrcjfsZNWsTU1fsQlW5vGl1bupYjwvPqUJUsfxTKjImHIKRLA4CdVT1SC4DiALWAFcAicB8YLCqrshgvWlAEvC+lyxqAjVVdZGIlAUWAtf6b+vPkkU+sGYqTLgDpBj0ew/O6RbpiDK1/eAJPp6zmdHzt7L/2Clqlo+lX+s4+rWJI75K6UiHZ0xYBGOmvBVApTzE0B5Yp6obVPUUMBrok8F6dwOfA7vTF6jqDlVd5D0/AqwEauchFhMuja+EYTOgbC34uJ8rbeTTNrJaFUrylx5NmPXIZbxxQ2ua1CjLmzPWcemLM+j/9izGzN/CkaTkSIdpTMQEWrJoALwDTOXs4T4+DGD764EeqnqH9/pmoIOq3uWzTm3gU+Ay4D1gkqqO99tPfeBnoIWqHs7gOMOAYQB169Zts3nz5mz/NhMGp47B13fD8s+hSW+49i2ILRfpqLK163ASExZtY/zCrazfc4yS0VH0bFGD69vE0bFBZYpZNZUpZLIqWQQ6AttQ4CKgIme2WSiuPSHbGDJY5p+lXgEeVtXUjLozikgZXKnj3owSBYCqjgBGgKuGCiAuEw4xpV01VO02MPUJeLcbDPwEqjaOdGRZql4ulju7NmT4JQ34betBxi9MZOKS7Uz4bRu1K5SkX5s4+reJo04laxQ3hV+gJYtDQEdVXZmrg4h0Ap5U1e7e60cBVPVZn3U28ntSqYKbN2OYqn4pItHAJOA7VX0pkGNam0U+tfEXdz9Gykno+xY0vTrSEeVIUnIq3yXsZPzCRGau24sqdIivRP+2dejZogalbQRcU4AFo4F7DXCBqh7LZQDFcQ3c3YBtuAbuG1Q1IZP1R+JVQ4krZowC9qvqvYEe05JFPnZoG4y9GbYt9MaVegyKFbz7HLYfPMEXv21j3IKtbNp3/PQIuP3b1rEb/kyBFIxkcSfQHXgOn8ZnAFXdEGAQvXBVTVG4nk7/FJHh3j7e9lt3JL8niy7AL8AyXNdZgL+q6uSsjmfJIp9LToJvH4JFH0LDbtDvXSiVlz4UkaOqLNx8gHELEvlm2Q6OnkyhYdXSDG5fl+tax1GpdEykQzQmIMFIFmmZvKWqmi9/ElqyKCAWjoTJD0HZmu5+jJrnRzqiPDl+KoVvlu7gs3lbWLTlIDFRxbiyeXUGt69LJ2sUN/mcDfdh8rfEBTDmZjhxAK5+FVoOjHREQbF65xFGz9/ChEXbOHQimXqVSzGwXR2bb8PkW5YsTP53dDeMu9XNj9F+GFz5DBQvEemogiIpOZUpy3fy2bwtzN24n+LFhG5NqzG4fV0ualTV7hQ3+UYwqqHqAn8n47Gh8mX/R0sWBVBqspvje/Z/odYF0H9kvhwmJC827DnKmPlbGb8wkX3HTlG7QkkGtK3DgHZxNnS6ibhgJIu5wCpgHGePDfVDMIIMNksWBdjKSfDlH93za98ocN1rA3EqJY1pK3Yxev4Wflm7l2ICXc91pY1Lz61K8ajIzDpoirZgJItDQEVVzayhO9+xZFHAHdjk7sfY/ht0/CNc/hQUL5y9irbuP86Y+VsZu2Aru4+cJK5iSe6+7Byuax1HtCUNE0bBSBYfA++p6vRgBxcqliwKgZST7o7vee+4u7+v/yBfzMIXKimpaXy/cjdvzVjHksRDljRM2AUjWVQCZgHrOXtsqNuCEWSwWbIoRBK+dGNLicC1b0OTXpGOKKRUlRmr9/DK92ssaZiwCsaosx8AqbgRX7f5PYwJrebXwh9+co3dowfD1MddY3ghJSJc2qQaX/7pQj4Y2o7KpWN4+PNlXPriDMbM30JyaoGpDTaFSKAliyNArdzOZxEJVrIohJKTYOpjMP9diGsP/T+A8nGRjirkrKRhwiUY1VC/Ajeq6qYgxxYyliwKseWfw9f3QFRx6DvCzZtRBFjSMKEWjGTxNDAQVx3l32bxfjCCDDZLFoXcvvUwdgjsWgYX3guXPeGSRxFgScOESjCSRWa9oFRVL8tLcKFiyaIISD4BUx5x40vV7QTXvw/lakU6qrCxpGGCzYb7MIXb0nEw8R6IjoXrRsA5l0c6orDyTxqVSsdwedNqdG9egwvPqUJsdL4c69PkQ0FNFt78EqcHs8mvN+pZsihi9q511VK7E+DCe6DrX13yKEJUlRlr9vDFom1MX7WbIydTKBUTRddzq9K9eQ26nluN8iWjIx2myceCUQ1VG/gvcDFQwfc9G6Lc5BunjsN3j7pqqSqNoc+bUKddpKOKiFMpaczesI/vEnYybcUu9hw5SXSU0LFBZbo3r8EVzapTvVzRSqYme8FIFhNx05w+C/yESxpPApNV9X/BCzV4LFkUYet+cNVSh7e5oUIuexyii+4gfWlpym9bDzI1YSffJexk077jAFxQtwLdm9fgymbVaVC1TDZ7MUVBMJLFPqCuqh4TkYOqWiH9rm5VbRJgED2AV3Ez5b2rqs9lsl47YA4wUFXH52RbX5Ysirikw/D932HB+1CpIfR5A+p1inRUEaeqrN19lO+W72Tqil0s23YIgEbVyrjE0bw659Uub1PCFlHBSBa7gTqqelJENgHtgMPAXlUtG8D2Ubg5uK8AEnFzcA9W1RUZrDcNSMJNvTo+0G39WbIwAGz4Cb6+Cw5uhQ5/gG5/g5jSkY4q39h28ATTEnbyXcIu5m3aT2qaUr1cCS5qVJWLG1flonOqUNGmhS0yskoWgXZMnwv0Ar4AvgPG4IYqD/TbuD2wLn2+bhEZDfQB/L/w7wY+xyWjnG5rzNkaXAJ3zoYfnoK5b8OaKXDNfyH+okhHli/UrlCSoRfGM/TCeA4cO8UPq3YzffVupq3YxfiFiYjA+XEVuKRRFS5uXJVWdSrY8OlFVKDJ4mZ+H0fqXuABoCzwSoDb1wa2+rxOBDr4ruA1ovcFLuPMZJHttj77GAYMA6hbt26AoZlCr0QZ6PVvaHYtfPUnGNUb2t3hhj0vYXX16SqWjuH6NnFc3yaO1DRlSeJBfl6zh5/X7OG/09fx2o/rKBtbnAsbusRxceMqxFUsFemwTZgElCxU9aDP8xPAMzk8TkYVoP71X68AD6tqql99aSDbpsc2AhgBrhoqhzGawq7+hXDnLPjxGZjzJqyZCte8Bg0vjXRk+U5UMaF13Yq0rluRey9vzKHjyfy6fi8/rd7Dz2v3MCVhJwANqpbmksauyqpjfGVKxuTLzpEmCLJMFiKS7fDjAQ73kQjU8XkdB2z3W6ctMNpLFFWAXiKSEuC2xgQmphT0+Bc06wNf/RE+uhZaD3FzfseWi3R0+Vb5UtH0Oq8mvc6riaqybvdRflqzh5/X7uXTuVv44NdNxBQvRvv6lejSqAqdGlSmRe3yNr94IZJlA3cGw3xcCPzq8zqg4T5EpDiukbobbljz+cANqpqQyfojgUleA3eOtk1nDdwmW8knYPq/3JzfZWvBNa8Wubu/gyEpOZW5G/efrrJau/soAGVLFKd9fCU6NaxMxwaVaVazHMUseeRruW7gVtUzyucicsB/WSBUNUVE7sI1jkfhejoliMhw7/23c7ptTmMw5izRJeHKp71Sxp/g437Q6ibo/k8oWSHS0RUYsdFRXNK4Kpc0rgrA7sNJzN6wjzkb9jF7/T5+WLUbgPIlo13yaFCZTg0rc271spY8CpAcDfchIvtVtVII4wkaK1mYHEk5CT89DzNfgTLV4KqXCv2MfOGy49CJ04lj9oZ9bN1/AoBKpWPo4JU8OjWozDnVytj9HREWtLGhLFmYQm/7b/DVXbBrObS4Hnq+AKUrRzqqQiXxwHHmbNjP7PWu9LHtoEseVcrE0KFBZS5uVIVuTatTpUyJCEda9FiyMCYnUk7Br6/ATy9AbHnX7bZ5XzcHuAkqVWXrfq/k4ZU+dh5OophA23qVuLJ5dbo3r0GdStZFNxxynSxEZCtndlOtjd+826qaL29osGRh8mzXCteWsX0RNOkNV/0HytaIdFSFmqqycscRvktww5Gs3HEYgCY1ytK9eQ26N69B05plrboqRPKSLC7Jbueq+lMeYgsZSxYmKFJT3D0Z0/8JxUtA92eh1Q1WygiTLfuOM3XFTqYm7GL+5v2oQp1KJbmymUscbepVtO65QWSTHxmTV3vXwdd3w5ZZ0LAbXP0qVKiT/XYmaPYePcn3K3YxdcUuZq7dy6nUNCqXjuHyptXp3qI6nRvaRE95ZcnCmGBIS4MF78G0v7uSxRVPQZvboJiNlRRuR0+mMGP1bqYm7Do90VPpmCi6nluNK5pVp+u5ValQygZAzClLFsYE04HNbr6MDdOhXhc3ZEjlhpGOqsg6mZLK7PX7mLpi1+mJnooJtKlXkcuaVKdb02o0sm65AbFkYUywqcJvH8N3j0HqKTfBUsc7oZhVg0RSWpqydNshfly5ix9W7SZhu2sgj6tYkm5NqnFZ0+p0iK9k1VWZsGRhTKgc3gGT7oM130Lttm6SpWoBzQdmwmDHoRNMX7WHH1ftYua6vSQlp1EqJoou51ShW9NqXHpuNarZ9LKnBWPyo0rAg0Ar4IwxnVX14iDEGHSWLEzYqMLyz2HyQ3DqKFz0AFx4L0Tbl1B+kpScyuwN+/hx5W5+XLX79M2A59Uuz2VNqtGtaTVa1CpfpIcgCUaymAKUAMbi5uI+TVVHBSPIYLNkYcLu6B6Y8rBLHBXjoefz0Lh7pKMyGVBVVu86wg9e4vhtywHSFKqWLcEVzarTr3VtWtetWOTaOYKRLA4DVVX1ZLCDCxVLFiZiNsxwpYy9a6BxT+j5HFSsH+moTBb2HzvFT2t28/3K3fy4cjcnklOpX7kU17WO47rWtYvMJE/BSBYzgSGquj7YwYWKJQsTUSmnYO5bMON50FToch9ceI8b6dbka0dPpvDtsh1MWLSN2Rv2AdCxQSX6tY6j53k1KVMi0AlGC55gJIt/AIOBD4Cdvu8FOPlR2FmyMPnCoW0w9XFImOBKFz2eh3N7RDoqE6DEA8f5YtE2Pl+UyKZ9xykZHUXPFjW4rnUcnRpWLnR3jwcjWfhPgpQuoMmPIsGShclXNvzkVU2thsY9oMdzUCk+0lGZAKkqi7YcYPzCbUxaup0jSSnULB9L3wtq069NHA2rFo653K3rrDH5QcopmPs2zHgO0lJc1VSXe61qqoBJSk5l2opdTFiUyE9r9pCm0KpOBfq1iePq82sW6DvHg5IsRKQicDW/jzw7UVUPBC3KILNkYfKtw9td1dTyz6FCPddr6tyekY7K5MLuI0l89dt2Pl+UyKqdR4iJKkaXRlVoVacCLetUoGVc+QKVPIJRDdUJ+AZYBWwG6gJNgatUdXaAQfQAXsVNjfquqj7n934f4GkgDUgB7lXVmd579wF34IZLXwbcqqpJWR3PkoXJ9zb+7Kqm9qyCRt1dr6lKDSIdlckFVSVh+2E+X5TIzLV7WbfnKOlfrfUrl6JlnQqcH1eBVnXK07xW+Xx7B3kwksVc4GVVHe2zbCDwoKq2C2D7KGANcAWQCMwHBqvqCp91ygDHVFVF5HxgrKo2EZHawEygmaqeEJGxwGRVHZnVMS1ZmAIhNfn3qqnUZFct1eU+q5oq4I4kJbNs2yGWbD3Ekq0HWZp4kO2H3O/bqGJCkxplTyePlnUq0Kha2XzRWJ5Vsgi0D1hj3A15vsYDbwe4fXtgnapu8AIaDfQBTicLVT3qs35pzpx0qThQUkSSgVLA9gCPa0z+FhUNne92U7hOfdzNA77kMzedq1VNFVhlY6Pp3LAKnRtWOb1s9+EkliS65LEk8SDfLN3OZ/O2AFAqJooWtcrTsk55LqhbkQ7xlaicz6aVDbRkMQ94RVU/9Vk2CFeyyDAL+W1/PdBDVe/wXt8MdFDVu/zW6ws8C1TDp4pLRO4B/gmcAKaq6o2ZHGcYMAygbt26bTZv3pzt32ZMvrLxF5j8oKuashv6CjVVZdO+4yzZepDFXgJJ2H6YUylpAJxbvSydGlamY4PKdGxQKSxtH8GohuoMTMJVJW0G6gONgN6qOiuA7fsD3f2SRXtVvTuT9S8G/qaql3sN658DA4GDwDhgvKp+nNUxrRrKFFipyW52vvQb+i56EC78s5upzxRqp1LSWL79ELPX72POhn3M37SfpOQ0RKBJjXJ0alCZTg0r0z6+EuVLRgf9+MHsDXUVUAtXDTRZVfcHuG0n4ElV7e69fhRAVZ/NYpuNQDvgUlyp5HZv+S1AR1X9Y1bHtGRhCrxD2+C7R2HFV1CpIfT6N5zTLdJRmTA6lZLGksSDzFm/j9kb9rFw8wFOpqRRTKB5rfJ0bFCJTg0r065+JcrG5j15RPw+CxEpjiuVdMN1u50P3KCqCT7rnAOs9xq4WwMTgThce8f7uMRxAhgJLFDV17M6piULU2is+8H1mtq/Hpr1cfOAl68d6ahMBCQlp7J468HTJY/fthzkVGoaUcWEFrXL08mrsrq4UdVcjZ6bq2QhIlNUtYf3/BfObHA+LdAhykWkF/AKruvs+6r6TxEZ7u3jbRF5GLgFSMYlhYd8us4+hauGSgF+A+7IblBDSxamUEk5CbNeg59fBImCrg9Dxz+6BnJTZCUlp7Jo8wFmb3DJY/HWg1QoFcO8v3bL1Yi5uU0WN6Q3aIvIkMx2bkOUGxNGBzbDlEdg9WSo2gR6vQjxF0U6KpNPHD+Vwpb9x2lSo1yutg9GA3cHVZ2bwfL2qjovV1GFmCULU6itngLfPgQHt8B5A+DKZ6Bs9UhHZQq4rJJFsQD3MS2T5VNyF5IxJk/O7QF/mgcX/wVWfAn/bQtz3oLUlEhHZgqpLJOFiBTz7r4WTzGfRyNcG4IxJhKiS8Jlj8Ef50BcO1c9NaIrbJkT6chMIZRdySIFOIW7azoF1/ic/lgBvBnS6Iwx2avcEG76HAZ8CCf2w/vdYewQ2L8x0pGZQiS74T7iAQF+Anx7PSmwR1VPhCowY0wOiLhutedcDrNeh19fdY3gHYbDxQ9CbPlIR2gKOJvPwpjC6PB2+PEZWPwplKoEXR+FNrdCVOGdEtTkXbDu4L4GuASogittAKCqtwQjyGCzZGEMsGMJfPcYbPoFqpzrek01usKVRIzxk+feUCLyd+Adb/3+wD6gO26sJmNMflWzJQyZCIM+c7PzfdofPuoLO5dHOjJTwATadfY24ApVvQ845f17NW5AQWNMfiYCTXq5XlM9noftv8E7F8HXd8ORXZGOzhQQgSaLCqqa/lPklIhEezfjXRKiuIwxwVY8BjoOhz//Bh3uhMWfweut4ed/Q7L1VTFZCzRZrBeR5t7z5cCd3jDj+XYObmNMJkpVgh7/gj/NhYaXuobw19vC0rGQlhbp6Ew+FWiyeByo7D1/FPgz8G/g/lAEZYwJg8oNYeDHMHQylK4CE/4P3u1mN/WZDFnXWWOMK1EsGwvfPwVHtkOrm+CKf0DpytlvawqNXM3BLSINAtl5+rzaxpgCrFgxaDkIml4NP70As/8Lq7+By5+EC25x75siLashytNwd2oLZ85lccZrVY0KZYC5ZSULY/Jg90r45gHY/CvEtYfeL0GN8yIdlQmxXN1noarFVDVKVYsBdwCjgSZArPfvp8DtIYjXGBNp1ZrC0G/g2rdh/wZ452KY8igkHY50ZCZCAi1bPo2bnW6tqp5S1bXAH4BnAj2QiPQQkdUisk5EHsng/T4islREFovIAhHp4vNeBREZLyKrRGSlN6e3MSaURKDVYLh7AbQZ6oZAf6M9LJ8AhbSt02Qu0GRRjLNvwKuHmyI1W94w528APYFmwGARaea32g9AS1VthbsJ8F2f914FpqhqE6AlsDLAuI0xeVWyIvR+Ge74AcpUg/G3wsfXwb71kY7MhFGgyeJl4EcR+ZeI3Cki/8J9ub8c4PbtgXWqukFVT+GqtPr4rqCqR/X3BpTSeO0iIlION+Lte956p1T1YIDHNcYES1wb+L/p0PPfkLgA3uwE05+F5KRIR2bCIKBkoar/Bm4FqgPXADWA21T1hQCPUxvY6vM60Vt2BhHpKyKrgG9wpQuABsAe4AMR+U1E3hWR0hkdRESGeVVYC/bs2RNgaMaYgBWLgg7D4K750Owa+Ok5eLMjrP0+0pGZEAu4P5yqTlHV21W1p6repqo5mVI1oyEuz6r0VNUvvKqma3HtJOC697YG3lLVC4BjwFltHt72I1S1raq2rVq1ag7CM8bkSNka0O9duOUrl0A+6Qdjb3FDo5tCKav7LB5T1X96z/+R2Xqq+rcAjpMI1PF5HQdkelWp6s8i0lBEqnjbJqrqXO/t8WSSLIwxYdagK9w5C2a9Bj+/COt+gC73Qbs7oGSFSEdngiirkkWcz/M6WTwCMR9oJCLxIhIDDAK+9l1BRM4RcYPsi0hrIAbYp6o7ga0icq63ajfclK7GmPygeAm4+CE3qm39i+DHp+HlFm4ejUOJkY7OBEnYhvsQkV7AK7geVO+r6j9FZDiAqr4tIg8Dt+Dm9z4BPKSqM71tW+F6R8UAG4BbVTXLQQztpjxjImTHEje16/IJrvtti+uh891Qo0WkIzPZyNVMeQV9uA9LFsZE2MEtMPtNWPQhJB+DhpdB5z+7qiubqS9fym2y8B3uIzNqw30YY7J04gDMfw/mvgPHdkON813SaN7X5gTPZ/I83EcWj3yZKIwx+UjJinDxg3DvMrj6NTfR0oQ74LVWruRx8mikIzQBsKEkjTHhER0LbYbAn+a5OcHLx8F3j8LLzbyh0W2K1/wsoAZuESkO/BE3jWoVfKqmVPXikEWXB1YNZUwBsHU+zHoVVk6CqGg4f6CroqraONKRFUm5qoby8zJu4MCfgTbA50A14MegRGiMKZrqtHOz9d29EC64CZaNc4MVjhsKuxIiHZ3xEWjJYhvQSVW3iMhBVa0gIk2Ad1T1kpBHmQtWsjCmADq6B+a8AfP+B6eOQpPerr2j1gWRjqxICEbJohS/j+10QkRKqeoqwD5BY0zwlKnqZue7dxlc8jBs/AVGdIVP+sPWeZGOrkgLNFmsBNp5zxcAT4rI48C2kERljCnaSlWCS/8K9y2Dyx53o9y+dwWMugY2zYx0dEVSlslCRNLfvwdI8Z7fjxvY72pgWOhCM8YUebHl3VAi9y6DK552072OvAre7wnrf7RJmMIoyzYLEdkBfAR8pKrLwhZVEFibhTGFUPIJWDgKfn0VjmyH2m3hkr9AoyvtrvAgyEubxXAgHpgnIotE5B4RsbG/jTGREV0SOg6HexbDVS/B0d3w6QAYcQmsnAhpaZGOsNDKMlmo6leq2h+oCbwD9MeNAPu1iPQTkehwBGmMMWcoXgLa3Q5/XgTX/BeSDsOYm+DtC2HJGJu9LwRyPOqsiMQDNwN3AKVUtUooAssrq4YypghJTYHln8MvL8LeNW6IkZaDofUQqNYk0tEVGFlVQ+VoFC8RKYHrFdUBN8XqrLyHZ4wxeRRVHFoOhPP6w8afYNEod6/GnDehTgeXNJr3hZhSkY60wAr0prwuuLkmBgC7cY3eH6rq5tCGl3tWsjCmiDu2FxZ/6hLHvnVQohycP8AljprnRzq6fClXQ5R7Gz6Jq3KqBIwDRqnqr6EIMtgsWRhjANe9dvMslzQSvoTUk+6O8NZD4LzroUTZSEeYb+SlN1RH4DGgpqoOy0uiEJEeIrJaRNaJyFlzaItIHxFZKiKLRWSBV5rxfT9KRH4TkUm5jcEYUwSJQP0L4boR8OBq6PkCpJyESffCi+fC13dD4kK7ZyMbYZlWVUSigDXAFUAibk7uwaq6wmedMsAxVVUROR8Yq6pNfN6/H2gLlFPV3tkd00oWxphMqbq7wheNdNO/Jh+H6i1caeP8/q6BvAgKxthQedUeWKeqG1T1FDAa6OO7gqoe1d8zV2ncLH0AiEgccBVuHm5jjMkbETfibZ834IHV0PtlKBYF3z4ELzaGz25wvatOHYt0pPlGuOY0rM3vAxGCK1108F9JRPoCz+KGP7/K561XgL8AVrlojAmu2HLQ9jb32L4Ylo6FhAmw+huILg3n9oQW/eCcbu7+jiIqXMkio/vwz6r/UtUvgC9E5GLgaeByEekN7FbVhSLSNcuDiAzDG6+qbt26eY3ZGFPU1GrlHlc+DVtmw7LxsOIrWD7ejVPV9GpocT3Uv6jIzR8erjaLTsCTqtrde/0ogKo+m8U2G3H3dDyA65GVAsQC5YAJqnpTVse0NgtjTFCkJsOGGa5aauUkOHUESld192206Adx7aFY4ZihOtddZ4MYQHFcA3c33LDm84EbVDXBZ51zgPVeA3drYCIQ59OOgVeyeNAauI0xEZF8AtZOcyWNNd9BShKUrwMtrnOJo8b5BXpAw6DdwZ1bqpoiIncB3wFRwPuqmiAiw7333wb6AbeISDJwAhio4chkxhgTqOiS0Owa90g6DKu/dYlj9htuJNzKjdw84m2GuomcCpGwlCwiwUoWxpiwOb7fa9v4HDb9AlExrm2j43Co2TLS0QUs4tVQkWDJwhgTEXvXwtx33FAjycegbifoMNzNJ57PG8UtWRhjTLglHYLfPnaJ4+Bm17bR7g5ofYubNjYfsmRhjDGRkpYKa6bA3Ldh489QvKQbIbfDcKjWNNLRnSHiDdzGGFNkFYuCJle5x64EV9JYMhoWjoT4S6DjndCoe77vfmslC2OMCbfj+12ymP8uHN4GFeOh/TC44CZ3R3mEWDWUMcbkR6kpsGoizHkbts6BmDLQ6gaXOKo0Cns4Vg1ljDH5UVRxdyd4876w/TdXRbVwJMwbAQ0vg/Z/gEZX5osqKitZGGNMfnJ0j0sYC96DIzu8Kqr/g1Y3QskKIT20VUMZY0xBk5oMKye6UsaW2RBdCloOcqWNak2y3z4XrBrKGGMKmqhob8yp62DHEpc0Fn8KC96H+Itd0ji3p+ttFQZWsjDGmILi+H43l/i8d+FwIpSvC+1uD9qNflYNZYwxhUlqCqye7Eobm36B4rFwXn/o8AeocV6ud2vVUMYYU5hEFf999NtdCS5pLBkDv30E9S6Em78I+qx+liyMMaYgq94crn4VLn/SjUW1d01Ipn+1ZGGMMYVByYrQ+e6Q7T7yd3oYY4zJ9yxZGGOMyVbYkoWI9BCR1SKyTkQeyeD9PiKyVEQWi8gCEeniLa8jItNFZKWIJIjIPeGK2RhjjBOWNgsRiQLeAK4AEoH5IvK1qq7wWe0H4GtVVRE5HxgLNAFSgAdUdZGIlAUWisg0v22NMcaEULhKFu2Bdaq6QVVPAaOBPr4rqOpR/f2mj9KAest3qOoi7/kRYCVQO0xxG2OMIXzJojaw1ed1Ihl84YtIXxFZBXwD3JbB+/WBC4C5GR1ERIZ5VVgL9uzZE4y4jTHGEL5kIRksO+vWcVX9QlWbANcCT5+xA5EywOfAvap6OKODqOoIVW2rqm2rVq2a96iNMcYA4UsWiUAdn9dxwPbMVlbVn4GGIlIFQESicYniE1WdEMpAjTHGnC0sY0OJSHFgDdAN2AbMB25Q1QSfdc4B1nsN3K2BibikAjAK2K+q9+bgmHuAzbkMuQqwN5fbhoPFlzcWX95YfHmTn+Orp6oZVsuEpTeUqqaIyF3Ad0AU8L6qJojIcO/9t4F+wC0ikgycAAZ6iaMLcDOwTEQWe7v8q6pOzuaYua6HEpEFmQ2mlR9YfHlj8eWNxZc3+T2+zIRtuA/vy32y37K3fZ4/DzyfwXYzybjNwxhjTJjYHdzGGGOyZckiYyMiHUA2LL68sfjyxuLLm/weX4YK7eRHxhhjgsdKFsYYY7JlycIYY0y2imyyCGAUXBGR17z3l3r3foQzvmxH2xWRriJyyBupd7GI/C3MMW4SkWXpIwVn8H7EzqGInOtzXhaLyGERuddvnbCePxF5X0R2i8hyn2WVRGSaiKz1/q2YybZZXq8hjO/fIrLK+/y+EJEKmWyb5bUQwvieFJFtPp9hr0y2jdT5G+MT2yaf7v/+24b8/OWZqha5B+5ej/VAAyAGWAI081unF/AtrttuR2BumGOsCbT2npfF3dToH2NXYFIEz+MmoEoW70f0HPp93jtxNxxF7PwBFwOtgeU+y14AHvGePwI8n0n8WV6vIYzvSqC49/z5jOIL5FoIYXxPAg8G8PlH5Pz5vf8f4G+ROn95fRTVkkW2o+B6rz9UZw5QQURqhitALRyj7Ub0HProhhsdILd39AeFumFs9vst7oMboQDv32sz2DSQ6zUk8anqVFVN8V7O4fdRFcIuk/MXiIidv3QiIsAA4LNgHzdcimqyCGQU3IBGyg2HbEbb7SQiS0TkWxFpHt7IUGCqiCwUkWEZvJ9fzuEgMv9PGsnzB1BdVXeA+4EAVMtgnfxyHm/DlRQzkt21EEp3edVk72dSjZcfzt9FwC5VXZvJ+5E8fwEpqskikFFwAxopN9Qk69F2F+GqVloCrwNfhjm8C1W1NdAT+JOIXOz3fsTPoYjEANcA4zJ4O9LnL1D54Tw+hpuI7JNMVsnuWgiVt4CGQCtgB66qx1/Ezx8wmKxLFZE6fwErqskikFFwczRSbihINqPtquphVT3qPZ8MRIs3Um84qOp279/dwBe44r6viJ9D3H++Raq6y/+NSJ8/z670qjnv390ZrBPR8ygiQ4DewI3qVbD7C+BaCAlV3aWqqaqaBvwvk+NG+vwVB64DxmS2TqTOX04U1WQxH2gkIvHeL89BwNd+63yNG9hQRKQjcCi9uiAcvDrO94CVqvpSJuvU8NZDRNrjPs99YYqvtLhpbhGR0riG0OV+q0X0HHoy/UUXyfPn42tgiPd8CPBVBusEcr2GhIj0AB4GrlHV45msE8i1EKr4fNvA+mZy3IidP8/lwCpVTczozUievxyJdAt7pB64njprcL0kHvOWDQeGe88FN2/4emAZ0DbM8XXBFZWXAou9Ry+/GO8CEnC9O+YAncMYXwPvuEu8GPLjOSyF+/Iv77MsYucPl7R2AMm4X7u3A5Vx88+v9f6t5K1bC5ic1fUapvjW4er706/Bt/3jy+xaCFN8H3nX1lJcAqiZn86ft3xk+jXns27Yz19eHzbchzHGmGwV1WooY4wxOWDJwhhjTLYsWRhjjMmWJQtjjDHZsmRhjDEmW5YsjMnHRERF5JxIx2GMJQtjcsAbSvqEiBz1efw30nEZE2rFIx2AMQXQ1ar6faSDMCacrGRhTBCIyFAR+VVEXhc3odIqEenm834tEflaRPZ7E/D8n897USLyVxFZLyJHvJFHfccyulzc5EgHROSN9CFKjAknK1kYEzwdgPFAFdzAcRNEJF5V9+OGgkjADfPQBJgmIhtU9QfgftwYVulDUpwP+I7D1BtoB5QDFgITgSlh+YuM8dhwH8bkgIhswiWDFJ/FD+HGA/oXUFu9/1QiMg839PkM3ExoFdRNZIWIPIsbx2ioiKwG/qKqZw0iKCIKXKSqM73XY3Gj6D4Xkj/QmExYNZQxOXetqlbwefzPW75Nz/z1tRlXkqgF7E9PFD7vpU/AUwc3wF1mdvo8Pw6UyVv4xuScJQtjgqe2X3tCXdy8CduBSunDUPu8t817vhU3gY8x+ZYlC2OCpxrwZxGJFpH+QFPcMNRbgVnAsyISKyLn44bXTp917l3gaRFp5M39cb6IVI7IX2BMJqyB25icmygiqT6vp+EmLZoLNAL2AruA61U1fTKlwcDbuFLGAeDvqjrNe+8loAQwFdcesgo3kY8x+YY1cBsTBCIyFLhDVbtEOhZjQsGqoYwxxmTLkoUxxphsWTWUMcaYbFnJwhhjTLYsWRhjjMmWJQtjjDHZsmRhjDEmW5YsjDHGZOv/AWNseWujW1FOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot the results for the constant threshold networks throughout training\n",
    "plt.plot(history_bpmll.history['val_hamming_loss'], label = 'BPMLL')\n",
    "plt.plot(history_ce.history['val_hamming_loss'], label = 'CE')\n",
    "plt.xlabel('Epoch', fontsize = 12)\n",
    "plt.ylabel('Validation Hamming Loss', fontsize = 12)\n",
    "plt.legend()\n",
    "plt.title('BPMLL vs Cross Entropy Training History', fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Constant Threshold</th>\n",
       "      <th>Learned Threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CE</th>\n",
       "      <td>0.325596</td>\n",
       "      <td>0.302228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BPMLL</th>\n",
       "      <td>0.376383</td>\n",
       "      <td>0.319442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Constant Threshold  Learned Threshold\n",
       "CE               0.325596           0.302228\n",
       "BPMLL            0.376383           0.319442"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Generate a dataframe with the validation hamming loss values using both constant and learned threshold values\n",
    "results_df = pd.DataFrame({'Constant Threshold' : [constant_ce, constant_bpmll], \n",
    "                        'Learned Threshold' : [learnedTF_ce, learnedTF_bpmll]})\n",
    "results_df.index = ['CE', 'BPMLL']\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We see here that performance with a learned threshold function improved upon performance with a constant $t(x) \\equiv 0.5$ threshold, for both the cross entropy and BP-MLL networks. You'll notice that the training time devoted to each network is very small. If we train for a larger number of epochs, performance using both loss functions and constant thresholds improves significantly, to the point where a learned threshold function provides no additional lift. This suggests that threshold function learning may be most useful in cases where the resources necessary to sufficiently train a model are prohibitive. \n",
    "\n",
    "Another observtion: with both learned and constant threshold functions, a standard binary cross entropy loss function outperformed the networks with the novel BP-MLL loss. This mirrors the results found in Nam et al. (2014), where potential explanations are also provided. Namely (1) the surface for the BP-MLL loss has plateaus in which gradient descent can be very slow in comparison with the cross-entropy loss function. And(2) while BPMLL is supposed to leverage correlations between labels, Nam et al. conjecture that these correlations may also cause overfitting. If groups of hidden units specialize in predicting particular label subsets that occur frequently in the training data, it will become harder to predict novel label combinations that only occur in the test set. Furthermore, another obstacle with BPMLL is that the surface of the objective function has plateaus in which gradient descent can be very slow in comparison with the cross-entropy loss function. \n",
    "\n",
    "The superior generalizability of cross entropy would, in fact, be a desirable outcome, for the following reason. Since computing the BP-MLL loss involves pairwise computations, obtaining error terms is more expensive than utilizing cross-entropy or MSE loss. This scales poorly with the number of labels, and can lead to significantly larger training times. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Jinseok Nam, Jungi Kim, Eneldo Loza Menc´ıa, Iryna Gurevych, and\n",
    "Johannes F¨urnkranz. Large-scale multi-label text classification —\n",
    "revisiting neural networks. In Toon Calders, Floriana Esposito, Eyke\n",
    "H¨ullermeier, and Rosa Meo, editors, Machine Learning and Knowledge\n",
    "Discovery in Databases, pages 437–452, Berlin, Heidelberg, 2014.\n",
    "Springer Berlin Heidelberg. ISBN 978-3-662-44851-9.\n",
    "\n",
    "* Min-Ling Zhang and Zhi-Hua Zhou. Ml-knn: A lazy learning approach to\n",
    "multi-label learning. Pattern Recognition, 40(7):2038–2048, 2007. doi:\n",
    "10.1016/j.patcog.2006.12.019.\n",
    "\n",
    "* Min-Ling Zhang and Zhi-Hua Zhou. Multilabel neural networks with\n",
    "applications to functional genomics and text categorization. IEEE\n",
    "Transactions on Knowledge and Data Engineering, 18(10):1338–1351,\n",
    "2006. doi: doi:10.1109/TKDE.2006.162.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
